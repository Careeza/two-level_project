{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "\ttransforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(y_min, y_max):\n",
    "\ttrain_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\ttest_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "\tX_train, y_train = next(iter(DataLoader(train_dataset, batch_size=len(train_dataset))))\n",
    "\tX_test, y_test = next(iter(DataLoader(test_dataset, batch_size=len(test_dataset))))\n",
    "\n",
    "\tX_train = X_train.view(-1, 28*28)\n",
    "\tX_test = X_test.view(-1, 28*28)\n",
    "\t# add a 1 to the end of each sample for the bias term\n",
    "\tX_train = torch.cat((X_train, torch.ones(X_train.shape[0], 1)), dim=1)\n",
    "\tX_test = torch.cat((X_test, torch.ones(X_test.shape[0], 1)), dim=1)\n",
    "\n",
    "\ty_train[y_train < 5] = y_min\n",
    "\ty_train[y_train >= 5] = y_max\n",
    "\ty_test[y_test < 5] = y_min\n",
    "\ty_test[y_test >= 5] = y_max\n",
    "\n",
    "\treturn X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss(y, x, theta):\n",
    "\ty_pred = x @ theta\n",
    "\ty_pred = y_pred.squeeze()\n",
    "\tloss = torch.log(1 + torch.exp(-y * y_pred)).mean()\n",
    "\treturn loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_linear_least_squares_with_sigmoid_loss(y, x, theta):\n",
    "\ty_pred = x @ theta\n",
    "\ty_pred = y_pred.squeeze()\n",
    "\tloss = (y - 1 / (1 + torch.exp(-y * y_pred))).mean()\n",
    "\treturn loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def line_search(f, x, w, y, loss, s, beta=0.5, c=1e-4, max_iter=16):\n",
    "\talpha = 1\n",
    "\tn_iter = 0\n",
    "\tloss = loss.item()\n",
    "\twhile f(y, x, w + alpha * s) >= loss + c * alpha * -(s.T @ s):\n",
    "\t\talpha *= beta\n",
    "\t\tif n_iter >= max_iter:\n",
    "\t\t\tbreak\n",
    "\t\tn_iter += 1\n",
    "\treturn alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def accuracy_func(y, x, theta):\n",
    "\ty_pred = x @ theta\n",
    "\ty_pred = torch.sign(y_pred).squeeze()\n",
    "\treturn (y_pred == y).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_level(X, y, loss_func, p, sample_size, n_epochs, verbose=False):\n",
    "\ttheta = torch.randn(X.shape[1], 1, dtype=torch.float32, requires_grad=True)\n",
    "\tX_h = X\n",
    "\ty_h = y\n",
    "\tLosses_h = []\n",
    "\tLosses_H = []\n",
    "\taccuracies = []\n",
    "\tdelta_fine_iter = []\n",
    "\tdelta_coarse_iter = []\n",
    "\ttime_start = time.time()\n",
    "\tfor epoch in range(n_epochs):\n",
    "\t\ttheta.grad = None\n",
    "\t\tloss_h = loss_func(y_h, X_h, theta)\n",
    "\n",
    "\t\tLosses_h.append(loss_h.item())\n",
    "\n",
    "\t\tloss_h.backward()\n",
    "\t\ts = -theta.grad\n",
    "\t\talpha = line_search(loss_func, X_h, theta, y_h, loss_h, s)\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\ttheta += alpha * s\n",
    "\n",
    "\t\tloss_after_fine = loss_func(y_h, X_h, theta)\n",
    "\t\tdelta_fine_iter.append(loss_after_fine - loss_h)\n",
    "\n",
    "\t\tidx = np.random.choice(X_h.shape[0], int(X_h.shape[0] * sample_size), replace=False)\n",
    "\t\tX_H = X_h[idx]\n",
    "\t\ty_H = y_h[idx]\n",
    "\t\ttheta_H = theta.clone().detach().requires_grad_(True)\n",
    "\t\tloss_H = loss_func(y_H, X_H, theta_H)\n",
    "\t\ttheta_H.grad = None\n",
    "\t\tloss_H.backward()\n",
    "\t\ts_H = -theta_H.grad\n",
    "\t\tV_H = -s - s_H\n",
    "\n",
    "\t\tLosses_H.append([])\n",
    "\n",
    "\t\tfor _ in range(p):\n",
    "\t\t\tloss_H = loss_func(y_H, X_H, theta_H) + V_H.T @ theta_H\n",
    "\n",
    "\t\t\tLosses_H[-1].append(loss_H.item())\n",
    "\n",
    "\t\t\ttheta_H.grad = None\n",
    "\t\t\tloss_H.backward()\n",
    "\t\t\ts_H = -theta_H.grad\n",
    "\t\t\talpha = line_search(lambda y, x, w: loss_func(y, x, w) + V_H.T @ w, X_H, theta_H, y_H, loss_H, s_H)\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\ttheta_H += alpha * s_H\n",
    "\t\ts_h = theta_H - theta\n",
    "\t\talpha = line_search(loss_func, X_h, theta, y_h, loss_after_fine, s_h)\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\ttheta += alpha * s_h\n",
    "\t\t\taccuracy = accuracy_func(y_h, X_h, theta)\n",
    "\t\t\taccuracies.append(accuracy.item())\n",
    "\t\tloss_after_coarse = loss_func(y_h, X_h, theta)\n",
    "\t\tdelta_coarse_iter.append(loss_after_coarse - loss_after_fine)\n",
    "\n",
    "\t\tif verbose and epoch % 10 == 0:\n",
    "\t\t\tprint(f'Epoch {epoch}/{n_epochs}, loss {loss_after_coarse.item()}, accuracy {accuracy.item()}')\n",
    "\n",
    "\tresult = {\n",
    "\t\t'Losses_h': Losses_h,\n",
    "\t\t'Losses_H': Losses_H,\n",
    "\t\t'accuracies': accuracies,\n",
    "\t\t'delta_fine_iter': delta_fine_iter,\n",
    "\t\t'delta_coarse_iter': delta_coarse_iter,\n",
    "\t\t'time': time.time() - time_start\n",
    "\t}\n",
    "\treturn theta, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = [1, 5, 10]\n",
    "sample_sizes = [0.1, 0.3, 0.5]\n",
    "losses = [logistic_loss]\n",
    "\n",
    "# ps = [5, 10, 20]\n",
    "# sample_sizes = [0.01, 0.05, 0.1]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for i, loss in enumerate(losses):\n",
    "\tX_train, y_train, X_test, y_test = get_data(-1, 1)\n",
    "\tfor p in ps:\n",
    "\t\tfor sample_size in sample_sizes:\n",
    "\t\t\ttheta, result = two_level(X_train, y_train, loss, p, sample_size, 100)\n",
    "\t\t\ttrain_accuracy = accuracy_func(y_train, X_train, theta).item()\n",
    "\t\t\ttest_accuracy = accuracy_func(y_test, X_test, theta).item()\n",
    "\t\t\tresults[f\"{p}_{sample_size}\"] = result\n",
    "\t\t\tprint(f\"p={p}, sample_size={sample_size}, train_accuracy={train_accuracy}, test_accuracy={test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i, p in enumerate(ps):\n",
    "\tfor j, sample_size in enumerate(sample_sizes):\n",
    "\t\tax = plt.subplot(len(ps), len(sample_sizes), i * len(sample_sizes) + j + 1)\n",
    "\t\tlosses_h = results[f\"{p}_{sample_size}\"]['Losses_h']\n",
    "\t\taccuracies = results[f\"{p}_{sample_size}\"]['accuracies']\n",
    "\t\tax.plot(losses_h, label=\"Loss_h\", color='blue')\n",
    "\t\tax.set_title(f\"p={p}, sample_size={sample_size}\")\n",
    "\t\tax.set_xlabel(\"Epoch\")\n",
    "\t\tax.set_ylabel(\"Loss\")\n",
    "\t\tax.legend()\n",
    "\t\tax2 = ax.twinx()\n",
    "\t\tax2.plot(accuracies, color='red', label=\"Accuracy\")\n",
    "\t\tax2.set_ylabel(\"Accuracy\")\n",
    "\t\tax2.set_ylim(0, 1)\n",
    "\t\tax2.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
